# Full Genome Large DNA Model

Yuri Kuratov, Veniamin Fishman, Olga Kardymon, Mikhail Burtsev

## Abstract

Genomics is a rapidly evolving field dedicated to unraveling the complexities of genome functionality through high-throughput, genome-wide assays. The intricate nature of genomic processes, coupled with the vast amounts of data generated by genomic techniques, makes this area particularly amenable to machine learning (ML)-based approaches for data analysis and interpretation. Such approaches have already yielded significant breakthroughs, including the prioritization of genomic variants for clinical assessments, the design of proteins with specific functions, and the advancement of genome editing technologies, among others. However, most existing ML solutions are tailored to address specific genomic challenges, often overlooking the interconnectedness of various genomic phenomena. In response, we will develop, train and release a series of foundational DNA language models designed to analyze DNA sequences up to billion base pairs. We anticipate that providing the genomics community with access to a pre-trained model will facilitate achieving state-of-the-art results across various downstream applications. Moreover, using a pretrained model will save computational resources. Consequently, this project will provide access to cutting-edge ML tools for research groups that do not have internal capacities to train large-scale models from scratch, thereby catalyzing advancements in genomic studies.

## Introduction and Prior Work

Recently, machine learning (ML) approaches have revolutionized genomics, demonstrating a remarkable ability to deduce a broad spectrum of epigenetic and genomic features directly from DNA sequences. These methodologies typically involve inputting a DNA sequence to predict specific features, such as protein binding sites, promoter activity, or splice donor and acceptor signals. Despite these advances, the current landscape of ML-based genomic tools faces several challenges. Firstly, most tools are designed to tackle individual genomic issues, neglecting the interconnected nature of molecular processes underlying DNA biochemistry. Secondly, the development of ML tools often demands substantial computational resources, rendering them out of reach for many researchers, especially those with limited access to data science infrastructure. Thirdly, certain architectures, such as transformers used for processing genomic sequences, have restrictions on the length of input they can handle; meanwhile, long-range interactions within DNA play a crucial role in numerous molecular functions. Lastly, the field of machine learning is continuously evolving, offering opportunities to enhance existing genomic tools with innovative, cutting-edge ML architectures from other fields.

An important direction of this research in this area is development of pre-trained transformed-based DNA language models. Beginning with the pioneering DNABERT transformer published in 2021 [[1](#1)], this approach was utilized to develop state-of-the-art solutions for genomic challenges. For example, pre-trained on the human genome, DNABERT demonstrated its ability to accurately predict promoter activity, splice site localization, and transcription factor binding sites after undergoing task-specific fine-tuning. The promising results achieved by DNABERT have spurred the rapid development of transformer-based pre-trained models, leading to the introduction of BigBird [[2](#2)], NucleotideTransformer [[3](#3)], DNABERT-2 [[4](#4)], and GENA DNA language models (developed by our group, [[5](#5)]). These newer models have expanded upon the capabilities of DNABERT, featuring an increased number of parameters and, crucially, the capacity to process longer input sequences. As we have shown recently [[6](#6)], this enhancement in handling extended sequences has proven to be vital for addressing a wide range of downstream genomic tasks. Notably, among the publicly available transformer-based pre-trained models, GENA-LMs boast the longest sequence input capability, accommodating up to 32 kb. This extended input range enables GENA-LMs to surpass DNABERT in nearly all biological tasks, show casing the significant impact of input sequence length on the performance of DNA language models in genomic research [[5](#5)].
 
Although GENA-LMs allow processing substantially longer DNA sequences than DNABERT, input capacities of these models are still below the scale of individual human genes, not to mention complete genomes. Recently, we proposed Recurrent Memory Transformers (RMT) as a novel solution to the challenge of extending the input length capacity of transformer models [[7](#7)]. Drawing inspiration from the principles of recurrent and memory networks, RMT sequentially processes lengthy inputs in chunks (or segments). To facilitate the transfer of information across these segments, RMT incorporates additional memory tokens into the standard BERT transformer architecture, enabling the accumulation and retention of information over segments. RMT benchmarking [[8](#8)] demonstrates the capability to handle extremely long inputs effectively. When integrated with GENA DNA language models (GENA-LMs), RMT has shown promise in various biological applications [[6](#6)]. However, in genomic contexts, the performance of RMT-augmented GENA-LMs with sequence lengths surpassing the original limits of GENA-LMs had not been thoroughly evaluated.


## Deliverables

What do we plan to provide the broader community with upon the completion of the project? Datasets? Models? APIs? Every deliverable should preferably have its own subsection with its associated potential impact, although it is not required.

### Datasets

We plan to employ the dataset initially introduced in the GENA-LMs paper [[5](#5)], with several enhancements. In summary, the dataset encompasses the most recent human reference genome, genomic variants from diverse human populations, and, optionally, genomes from another species. As part of our preliminary efforts, we have addressed various technical challenges in data processing and are in the process of incorporating genomic variants from broader human population studies. Our strategy involves preparing dataset options that vary in the ratio of human to non-human data and in the representation of population-specific variants. These datasets will undergo preliminary small-scale evaluations to determine the most effective configuration, which will then be utilized in a large-scale model training. Given our prior experience in constructing similar datasets, we are confident in our ability to efficiently complete this phase of the project.

### Models

We aim to develop an encoder-only transformer-based DNA language model that processes tokenized DNA sequences using Byte Pair Encoding (BPE) tokenization. The model will undergo training via the masked language modeling (MLM) task. 

Our plan includes the development of various small-scale models, each differentiated by hyperparameters, input sequence length, tokenizer parameters, and the quantity of Recurrent Memory Transformer (RMT) segments utilized. We acknowledge the critical role of the training schedule in the successful development of RMT models. Our previous experiments [[8](#8)] have demonstrated the efficacy of curriculum learning in RMT training. This method starts with RMT training on a limited number of segments. As the model achieves a point of convergence, the sequence length is progressively increased by adding more segments. Such an approach allows the RMT to gradually adapt to processing longer sequences, thereby improving its efficiency in managing extended inputs. Consequently, we will develop models pretrained with varying numbers of RMT segments.

Initial small-scale trials will be conducted to identify optimal model hyperparameters. Following this, we will focus on developing a final model capable of processing billions of base pairs with approximately 7 billion parameters. We plan to develop and make available models that are pre trained both with and without the RMT.

### APIs

Not applicable.

### Paper

We expect that successful realization of the project will result in a scientific paper published in A* ML conference  and/or Q1 journal. The blog post and other media promotion will be appreciated, when both contribution of author’s affiliations and stability.ai acknowledged.

## Resources

### Requirements

The project requires approximately 200 000 GPU hours (NVIDIA  A100/H100 80 Gb or equivalent), 256 GB RAM, 10 Tb storage to train 7 billion model. About the 150 000 - 200 000 GPU hours are estimated for training smaller models and benchmarking.

### Timeline

We expect that it would take:

* ~1 month to finish dataset preparation
* ~2 month to tweak model and dataset parameters
* ~1 month (if ~256 GPU are available simultaneously) to develop the final model
* ~2 month for benchmarks and downstream tasks evaluations 

## Broader Impact

While leading transformer models in natural language processing (NLP) boast up to billions or even hundreds of billions of parameters, the most extensive GENA-LM model encompasses only 350 million parameters. Expanding the scale of the model, coupled with extending the context up to billion base pair input sequences, would enable the detection of more intricate dependencies within genomic sequences, enhancing the model's predictive power and applicability in genomics.

## Reproducibility

The code required for dataset generation and model training will be released together with the model. The scientific paper will be prepared to describe details of model development and evaluation.

## Failure Case

Several scenarios can be considered. 
Increasing sequence length with RMT does not improve model performance. In this case, we can deliver a non-RMT (one-segment) model which has more parameters than previously published DNA transformers.
Increasing the number of parameters does not provide better model quality. This will be also important for the community, because currently leaders of the field [[3](#3)],[[4](#4)] focus their resources on increasing the number of model’s parameters.

## Preliminary Findings

We have published several papers describing DNA language models [[5](#5)], RMT technique [[7](#7)], and how these methodologies can be combined to solve genomic challenges [[6](#6)]. 

## Next Steps

We foresee several important downstream tasks that can be solved using the developed models. Among the key applications are the functional interpretation of non-coding variants within the human genome, the annotation of genes in newly assembled genomes, and the design of synthetic promoters tailored to exhibit specific activity profiles. These examples represent just a fraction of the potential applications, highlighting the models' capacity to significantly advance our understanding and manipulation of genomic information.

## Known contributors

This project will be executed in close collaboration with the developers of GENA-LMs. The primary team hails from the AIRI institute, comprising Yurii Kuratov, Alexey Shmelev, Dmitry Penzar, and Olga Kardymon. Additionally, we have contributors from the London Institute of Mathematical Sciences, represented by Mikhail Burtsev, and from the Institute of Cytology and Genetics, including Veniamin Fishman and Andrey Popov.

# References

[<a name="1">1</a>]. [Ji, Yanrong, et al. "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome." Bioinformatics 37.15 (2021): 2112-2120.](https://academic.oup.com/bioinformatics/article-abstract/37/15/2112/6128680)

[<a name="2">2</a>]. [Zaheer, Manzil, et al. "Big bird: Transformers for longer sequences." Advances in neural information processing systems 33 (2020): 17283-17297.](https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf)

[<a name="3">3</a>]. [Dalla-Torre, Hugo, et al. "The nucleotide transformer: Building and evaluating robust foundation models for human genomics." bioRxiv (2023): 2023-01.](https://www.biorxiv.org/content/10.1101/2023.01.11.523679.abstract)

[<a name="4">4</a>]. [Zhou, Zhihan, et al. "Dnabert-2: Efficient foundation model and benchmark for multi-species genome." arXiv preprint arXiv:2306.15006 (2023).](https://arxiv.org/abs/2306.15006)

[<a name="5">5</a>]. [Fishman, Veniamin, et al. "GENA-LM: A Family of Open-Source Foundational DNA Language Models for Long Sequences." bioRxiv (2023): 2023-06.](https://www.biorxiv.org/content/10.1101/2023.06.12.544594.abstract)

[<a name="6">6</a>]. [Kuratov, Yuri, et al. "Recurrent memory augmentation of GENA-LM improves performance on long DNA sequence tasks." ICLR 2024 Workshop on Machine Learning for Genomics Explorations.](https://openreview.net/forum?id=K671lCX90x)

[<a name="7">7</a>]. [Bulatov, Aydar, Yury Kuratov, and Mikhail Burtsev. "Recurrent memory transformer." Advances in Neural Information Processing Systems 35 (2022): 11079-11091.](https://proceedings.neurips.cc/paper_files/paper/2022/hash/47e288629a6996a17ce50b90a056a0e1-Abstract-Conference.html)

[<a name="2">8</a>]. [Bulatov, Aydar, Yuri Kuratov, and Mikhail S. Burtsev. "Scaling transformer to 1m tokens and beyond with rmt." arXiv preprint arXiv:2304.11062 (2023).](https://arxiv.org/abs/2304.11062)
